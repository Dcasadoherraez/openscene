{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39785a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataloader for fused point features.'''\n",
    "import sys\n",
    "sys.path.append('/home/daniel/spatial_understanding/benchmarks/openscene/')\n",
    "import copy\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import torch\n",
    "import numpy as np\n",
    "import SharedArray as SA\n",
    "from dataset.point_loader import Point3DLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40c0b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection_mapping = {\n",
    "#     0  'animal': 'animal',\n",
    "#     1  'movable_object.barrier': 'barrier',\n",
    "#     2  'vehicle.bicycle': 'bicycle',\n",
    "#     3  'vehicle.bus.bendy': 'bus',\n",
    "#     4  'vehicle.bus.rigid': 'bus',\n",
    "#     5  'vehicle.car': 'car',\n",
    "#     6  'vehicle.motorcycle': 'motorcycle',\n",
    "#     7  'vehicle.construction': 'other_vehicle',\n",
    "#     8  'vehicle.other': 'other_vehicle',\n",
    "#     9  'human.pedestrian.adult': 'pedestrian',\n",
    "#     10 'human.pedestrian.child': 'pedestrian',\n",
    "#     11 'human.pedestrian.construction_worker': 'pedestrian',\n",
    "#     12 'human.pedestrian.police_officer': 'pedestrian',\n",
    "#     13 'movable_object.trafficcone': 'traffic_cone',\n",
    "#     14 'static_object.traffic_sign': 'traffic_sign',\n",
    "#     15 'vehicle.ego_trailer': 'trailer',\n",
    "#     16 'vehicle.trailer': 'trailer',\n",
    "#     17 'vehicle.truck': 'truck'\n",
    "# }\n",
    "\n",
    "\n",
    "TRUCKSCENES_LABELS_TO_IDX = {\"animal\": 0 ,\n",
    "                            \"human.pedestrian.adult\": 1 ,\n",
    "                            \"human.pedestrian.child\": 2 ,\n",
    "                            \"human.pedestrian.construction_worker\": 3 ,\n",
    "                            \"human.pedestrian.personal_mobility\": 4 ,\n",
    "                            \"human.pedestrian.police_officer\": 5 ,\n",
    "                            \"human.pedestrian.stroller\": 6 ,\n",
    "                            \"human.pedestrian.wheelchair\": 7 ,\n",
    "                            \"movable_object.barrier\": 8 ,\n",
    "                            \"movable_object.debris\": 9 ,\n",
    "                            \"movable_object.pushable_pullable\": 10,\n",
    "                            \"movable_object.trafficcone\": 11,\n",
    "                            \"static_object.bicycle_rack\": 12,\n",
    "                            \"static_object.traffic_sign\": 13,\n",
    "                            \"vehicle.bicycle\": 14,\n",
    "                            \"vehicle.bus.bendy\": 15,\n",
    "                            \"vehicle.bus.rigid\": 16,\n",
    "                            \"vehicle.car\": 17,\n",
    "                            \"vehicle.construction\": 18,\n",
    "                            \"vehicle.emergency.ambulance\": 19,\n",
    "                            \"vehicle.emergency.police\": 20,\n",
    "                            \"vehicle.motorcycle\": 21,\n",
    "                            \"vehicle.trailer\": 22,\n",
    "                            \"vehicle.truck\": 23,\n",
    "                            \"vehicle.train\": 24,\n",
    "                            \"vehicle.other\": 25,\n",
    "                            \"vehicle.ego_trailer\": 26,\n",
    "                            \"unlabeled\": 27\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "TRUCKSCENES_CLASS_REMAP = 256*np.ones(28) # map from 28 classes to 16 classes\n",
    "TRUCKSCENES_CLASS_REMAP[0] = 0 # animal\n",
    "TRUCKSCENES_CLASS_REMAP[1] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[2] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[3] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[4] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[5] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[6] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[7] = 9 # pedestrian\n",
    "TRUCKSCENES_CLASS_REMAP[8] = 1 # barrier\n",
    "TRUCKSCENES_CLASS_REMAP[11] = 13 # traffic cone\n",
    "TRUCKSCENES_CLASS_REMAP[13] = 14 # traffic sign\n",
    "TRUCKSCENES_CLASS_REMAP[14] = 2 # bicycle\n",
    "TRUCKSCENES_CLASS_REMAP[15] = 3 # bus\n",
    "TRUCKSCENES_CLASS_REMAP[16] = 3 # bus\n",
    "TRUCKSCENES_CLASS_REMAP[17] = 5 # car\n",
    "TRUCKSCENES_CLASS_REMAP[18] = 7 # construction vehicle\n",
    "TRUCKSCENES_CLASS_REMAP[21] = 6 # motorcycle\n",
    "TRUCKSCENES_CLASS_REMAP[22] = 16 # trailer\n",
    "TRUCKSCENES_CLASS_REMAP[23] = 17 # truck\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71f64354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FusedFeatureLoader(Point3DLoader):\n",
    "    '''Dataloader for fused point features.'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 datapath_prefix, # data_root\n",
    "                 datapath_prefix_feat, # data_root_2d_fused_feature\n",
    "                 voxel_size=0.05,\n",
    "                 split='train', aug=False, memcache_init=False,\n",
    "                 identifier=7791, loop=1, eval_all=False,\n",
    "                 input_color = False,\n",
    "                 ):\n",
    "        super().__init__(datapath_prefix=datapath_prefix, voxel_size=voxel_size,\n",
    "                                           split=split, aug=aug, memcache_init=memcache_init,\n",
    "                                           identifier=identifier, loop=loop,\n",
    "                                           eval_all=eval_all, input_color=input_color)\n",
    "        self.aug = aug\n",
    "        self.input_color = input_color # decide whether we use point color values as input\n",
    "\n",
    "        # prepare for 3D features\n",
    "        self.datapath_feat = datapath_prefix_feat\n",
    "\n",
    "        # Precompute the occurances for each scene\n",
    "        # for training sets, ScanNet and Matterport has 5 each, nuscene 1\n",
    "        # for evaluation/test sets, all has just one\n",
    "        if 'nuscenes' in self.dataset_name or 'truckscenes' in self.dataset_name: # only one file for each scene\n",
    "            self.list_occur = None\n",
    "        \n",
    "        if len(self.data_paths) == 0:\n",
    "            raise Exception('0 file is loaded in the feature loader.')\n",
    "\n",
    "    def __getitem__(self, index_long):\n",
    "\n",
    "        index = index_long % len(self.data_paths)\n",
    "        if self.use_shm:\n",
    "            locs_in = SA.attach(\"shm://%s_%s_%06d_locs_%08d\" % (\n",
    "                self.dataset_name, self.split, self.identifier, index)).copy()\n",
    "            feats_in = SA.attach(\"shm://%s_%s_%06d_feats_%08d\" % (\n",
    "                self.dataset_name, self.split, self.identifier, index)).copy()\n",
    "            labels_in = SA.attach(\"shm://%s_%s_%06d_labels_%08d\" % (\n",
    "                self.dataset_name, self.split, self.identifier, index)).copy()\n",
    "        else:\n",
    "            locs_in, feats_in, labels_in = torch.load(self.data_paths[index])\n",
    "            \n",
    "            if 'truckscenes' in self.dataset_name:\n",
    "                labels_in = [TRUCKSCENES_LABELS_TO_IDX[l] if l in TRUCKSCENES_LABELS_TO_IDX else 255 for l in labels_in]\n",
    "                labels_in = np.array([TRUCKSCENES_CLASS_REMAP[l] for l in labels_in])\n",
    "                locs_in = np.ascontiguousarray(locs_in[:, :3])\n",
    "            else:\n",
    "                labels_in[labels_in == -100] = 255\n",
    "\n",
    "            labels_in = labels_in.astype(np.uint8)\n",
    "            if np.isscalar(feats_in) and feats_in == 0:\n",
    "                # no color in the input point cloud, e.g nuscenes lidar\n",
    "                feats_in = np.zeros_like(locs_in)\n",
    "            else:\n",
    "                feats_in = (feats_in + 1.) * 127.5\n",
    "\n",
    "        # load 3D features\n",
    "        if self.dataset_name == 'scannet_3d':\n",
    "            scene_name = self.data_paths[index][:-15].split('/')[-1]\n",
    "        else:\n",
    "            scene_name = self.data_paths[index][:-4].split('/')[-1]\n",
    "            \n",
    "        print(scene_name)\n",
    "\n",
    "        if 'nuscenes' not in self.dataset_name and 'truckscenes' not in self.dataset_name:\n",
    "            n_occur = self.list_occur[index]\n",
    "            if n_occur > 1:\n",
    "                nn_occur = np.random.randint(n_occur)\n",
    "            elif n_occur == 1:\n",
    "                nn_occur = 0\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            processed_data = torch.load(join(\n",
    "                self.datapath_feat, scene_name+'_%d.pt'%(nn_occur)))\n",
    "        else:\n",
    "            # no repeated file\n",
    "            processed_data = torch.load(join(self.datapath_feat, scene_name+'.pt'))\n",
    "\n",
    "        flag_mask_merge = False\n",
    "        if len(processed_data.keys())==2:\n",
    "            flag_mask_merge = True\n",
    "            feat_3d, mask_chunk = processed_data['feat'], processed_data['mask_full']\n",
    "            if isinstance(mask_chunk, np.ndarray): # if the mask itself is a numpy array\n",
    "                mask_chunk = torch.from_numpy(mask_chunk)\n",
    "            mask = copy.deepcopy(mask_chunk)\n",
    "            if self.split != 'train': # val or test set\n",
    "                feat_3d_new = torch.zeros((locs_in.shape[0], feat_3d.shape[1]), dtype=feat_3d.dtype)\n",
    "                feat_3d_new[mask] = feat_3d\n",
    "                feat_3d = feat_3d_new\n",
    "                mask_chunk = torch.ones_like(mask_chunk) # every point needs to be evaluted\n",
    "        elif len(processed_data.keys())>2: # legacy, for old processed features\n",
    "            feat_3d, mask_visible, mask_chunk = processed_data['feat'], processed_data['mask'], processed_data['mask_full']\n",
    "            mask = torch.zeros(feat_3d.shape[0], dtype=torch.bool)\n",
    "            mask[mask_visible] = True # mask out points without feature assigned\n",
    "\n",
    "        if len(feat_3d.shape)>2:\n",
    "            feat_3d = feat_3d[..., 0]\n",
    "\n",
    "        locs = self.prevoxel_transforms(locs_in) if self.aug else locs_in\n",
    "\n",
    "        # calculate the corresponding point features after voxelization\n",
    "        if self.split == 'train' and flag_mask_merge:\n",
    "            locs, feats, labels, inds_reconstruct, vox_ind = self.voxelizer.voxelize(\n",
    "                locs_in, feats_in, labels_in, return_ind=True)\n",
    "            vox_ind = torch.from_numpy(vox_ind)\n",
    "            mask = mask_chunk[vox_ind] # voxelized visible mask for entire point cloud\n",
    "            mask_ind = mask_chunk.nonzero(as_tuple=False)[:, 0]\n",
    "            index1 = - torch.ones(mask_chunk.shape[0], dtype=int)\n",
    "            index1[mask_ind] = mask_ind\n",
    "\n",
    "            index1 = index1[vox_ind]\n",
    "            chunk_ind = index1[index1!=-1]\n",
    "\n",
    "            index2 = torch.zeros(mask_chunk.shape[0])\n",
    "            index2[mask_ind] = 1\n",
    "            index3 = torch.cumsum(index2, dim=0, dtype=int)\n",
    "            # get the indices of corresponding masked point features after voxelization\n",
    "            indices = index3[chunk_ind] - 1\n",
    "\n",
    "            # get the corresponding features after voxelization\n",
    "            feat_3d = feat_3d[indices]\n",
    "        elif self.split == 'train' and not flag_mask_merge: # legacy, for old processed features\n",
    "            feat_3d = feat_3d[mask] # get features for visible points\n",
    "            locs, feats, labels, inds_reconstruct, vox_ind = self.voxelizer.voxelize(\n",
    "                locs_in, feats_in, labels_in, return_ind=True)\n",
    "            mask_chunk[mask_chunk.clone()] = mask\n",
    "            vox_ind = torch.from_numpy(vox_ind)\n",
    "            mask = mask_chunk[vox_ind] # voxelized visible mask for entire point clouds\n",
    "            mask_ind = mask_chunk.nonzero(as_tuple=False)[:, 0]\n",
    "            index1 = - torch.ones(mask_chunk.shape[0], dtype=int)\n",
    "            index1[mask_ind] = mask_ind\n",
    "\n",
    "            index1 = index1[vox_ind]\n",
    "            chunk_ind = index1[index1!=-1]\n",
    "\n",
    "            index2 = torch.zeros(mask_chunk.shape[0])\n",
    "            index2[mask_ind] = 1\n",
    "            index3 = torch.cumsum(index2, dim=0, dtype=int)\n",
    "            # get the indices of corresponding masked point features after voxelization\n",
    "            indices = index3[chunk_ind] - 1\n",
    "\n",
    "            # get the corresponding features after voxelization\n",
    "            feat_3d = feat_3d[indices]\n",
    "        else:\n",
    "            locs, feats, labels, inds_reconstruct, vox_ind = self.voxelizer.voxelize(\n",
    "                locs[mask_chunk], feats_in[mask_chunk], labels_in[mask_chunk], return_ind=True)\n",
    "            vox_ind = torch.from_numpy(vox_ind)\n",
    "            feat_3d = feat_3d[vox_ind]\n",
    "            mask = mask[vox_ind]\n",
    "\n",
    "        if self.eval_all: # during evaluation, no voxelization for GT labels\n",
    "            labels = labels_in\n",
    "        if self.aug:\n",
    "            locs, feats, labels = self.input_transforms(locs, feats, labels)\n",
    "        coords = torch.from_numpy(locs).int()\n",
    "        coords = torch.cat((torch.ones(coords.shape[0], 1, dtype=torch.int), coords), dim=1)\n",
    "        if self.input_color:\n",
    "            feats = torch.from_numpy(feats).float() / 127.5 - 1.\n",
    "        else:\n",
    "            # hack: directly use color=(1, 1, 1) for all points\n",
    "            feats = torch.ones(coords.shape[0], 3)\n",
    "        labels = torch.from_numpy(labels).long()\n",
    "\n",
    "        if self.eval_all:\n",
    "            return coords, feats, labels, feat_3d, mask, torch.from_numpy(inds_reconstruct).long()\n",
    "        return coords, feats, labels, feat_3d, mask\n",
    "\n",
    "def collation_fn(batch):\n",
    "    '''\n",
    "    :param batch:\n",
    "    :return:    coords: N x 4 (batch,x,y,z)\n",
    "                feats:  N x 3\n",
    "                labels: N\n",
    "                colors: B x C x H x W x V\n",
    "                labels_2d:  B x H x W x V\n",
    "                links:  N x 4 x V (B,H,W,mask)\n",
    "\n",
    "    '''\n",
    "    coords, feats, labels, feat_3d, mask_chunk = list(zip(*batch))\n",
    "\n",
    "    for i in range(len(coords)):\n",
    "        coords[i][:, 0] *= i\n",
    "\n",
    "    return torch.cat(coords), torch.cat(feats), torch.cat(labels), \\\n",
    "        torch.cat(feat_3d), torch.cat(mask_chunk)\n",
    "\n",
    "\n",
    "def collation_fn_eval_all(batch):\n",
    "    '''\n",
    "    :param batch:\n",
    "    :return:    coords: N x 4 (x,y,z,batch)\n",
    "                feats:  N x 3\n",
    "                labels: N\n",
    "                colors: B x C x H x W x V\n",
    "                labels_2d:  B x H x W x V\n",
    "                links:  N x 4 x V (B,H,W,mask)\n",
    "                inds_recons:ON\n",
    "\n",
    "    '''\n",
    "    coords, feats, labels, feat_3d, mask, inds_recons = list(zip(*batch))\n",
    "    inds_recons = list(inds_recons)\n",
    "\n",
    "    accmulate_points_num = 0\n",
    "    for i in range(len(coords)):\n",
    "        coords[i][:, 0] *= i\n",
    "        inds_recons[i] = accmulate_points_num + inds_recons[i]\n",
    "        accmulate_points_num += coords[i].shape[0]\n",
    "\n",
    "    return torch.cat(coords), torch.cat(feats), torch.cat(labels), \\\n",
    "        torch.cat(feat_3d), torch.cat(mask), torch.cat(inds_recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41a7c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "dataroot = \"/home/daniel/spatial_understanding/benchmarks/openscene/data\"\n",
    "\n",
    "dataloader = FusedFeatureLoader(datapath_prefix=f\"{dataroot}/truckscenes_3d\",\n",
    "                                datapath_prefix_feat=f\"{dataroot}/truckscenes_multiview_openseg_{split}\",\n",
    "                                voxel_size=0.01, \n",
    "                                split=split, aug=False,\n",
    "                                memcache_init=False, eval_all=True, identifier=6797,\n",
    "                                input_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47d09045",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 255 is out of bounds for axis 0 with size 28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 46\u001b[0m, in \u001b[0;36mFusedFeatureLoader.__getitem__\u001b[0;34m(self, index_long)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruckscenes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name:\n\u001b[1;32m     45\u001b[0m     labels_in \u001b[38;5;241m=\u001b[39m [TRUCKSCENES_LABELS_TO_IDX[l] \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m TRUCKSCENES_LABELS_TO_IDX \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels_in]\n\u001b[0;32m---> 46\u001b[0m     labels_in \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([TRUCKSCENES_CLASS_REMAP[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels_in])\n\u001b[1;32m     47\u001b[0m     locs_in \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(locs_in[:, :\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[52], line 46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruckscenes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name:\n\u001b[1;32m     45\u001b[0m     labels_in \u001b[38;5;241m=\u001b[39m [TRUCKSCENES_LABELS_TO_IDX[l] \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m TRUCKSCENES_LABELS_TO_IDX \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels_in]\n\u001b[0;32m---> 46\u001b[0m     labels_in \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mTRUCKSCENES_CLASS_REMAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels_in])\n\u001b[1;32m     47\u001b[0m     locs_in \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(locs_in[:, :\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 255 is out of bounds for axis 0 with size 28"
     ]
    }
   ],
   "source": [
    "dataloader[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openscene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
